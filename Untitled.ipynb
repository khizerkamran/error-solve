{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a324dcfc",
   "metadata": {},
   "source": [
    "VGG16 is a convolution neural net (CNN ) architecture which was used to win ILSVR(Imagenet) competition in 2014. It is considered to be one of the excellent vision model architecture till date. ... It follows this arrangement of convolution and max pool layers consistently throughout the whole architecture.\n",
    "The Convolutional Neural Network is then instantiated on Line 59 using the pre-trained ImageNet weights; Note: Weights for VGG16 and VGG19 are > 500MB. ResNet weights are ~100MB, while Inception and Xception weights are between 90-100MB.\n",
    "When loading a given model, the “include_top” argument can be set to False, in which case the fully-connected output layers of the model used to make predictions is not loaded, allowing a new output layer to be added and trained.\n",
    "Include_top lets you select if you want the final dense layers or not. the convolutional layers work as feature extractors. They identify a series of patterns in the image, and each layer can identify more elaborate patterns by seeing patterns of patterns\n",
    "\n",
    "Here the second warning message is not displayed due to warnings. filterwarnings('ignore', '. *do not. *', ) in which action is \"ignore\" . showwarning(message, category, filename, lineno, file=None, line=None): This function Writes a warning to a file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aa387a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 50178     \n",
      "=================================================================\n",
      "Total params: 14,764,866\n",
      "Trainable params: 50,178\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "522/522 [==============================] - 1784s 3s/step - loss: 0.2709 - accuracy: 0.8952 - val_loss: 0.2938 - val_accuracy: 0.9183\n",
      "Result is Normal\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow import keras\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "train_path = 'Datasets/train'\n",
    "valid_path = 'Datasets/test'\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "folders = glob('Datasets/train/*')\n",
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "# create a model object\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "# view the structure of the model\n",
    "model.summary()\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('Datasets/train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 10,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('Datasets/test',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 10,\n",
    "                                            class_mode = 'categorical')\n",
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=1,\n",
    "  steps_per_epoch=len(training_set),\n",
    "  validation_steps=len(test_set)\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "\n",
    "model.save('chest_xray.h5')\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "model=load_model('chest_xray.h5')\n",
    "img=image.load_img('C:\\\\Chest_x_ray_Detection-master\\\\Datasets\\\\val\\\\NORMAL\\\\NORMAL2-IM-1431-0001.jpeg',target_size=(224,224))\n",
    "x=image.img_to_array(img)\n",
    "#Expand the shape of an array. \n",
    "#Insert a new axis that will appear at the axis position in the expanded array shape.\n",
    "x=np.expand_dims(x, axis=0)\n",
    "#The preprocess_input function is meant to adequate your image to the format the model requires. Some models use images with values ranging from 0 to 1. Others from -1 to +1. \n",
    "#Others use the \"caffe\" style, that is not normalized, but is centered\n",
    "img_data=preprocess_input(x)\n",
    "\n",
    "classes=model.predict(img_data)\n",
    "result=int(classes[0][0])\n",
    "\n",
    "if result==0:\n",
    "    print(\"Person is Affected By PNEUMONIA\")\n",
    "else:\n",
    "    print(\"Result is Normal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a1540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
